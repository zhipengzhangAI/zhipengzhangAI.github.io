<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Zhipeng Zhang</title>
    <meta name="author" content="Zhipeng Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
    <table style="width:100%;max-width:800px;border:0;border-spacing:0;margin:auto;">
        <tbody>
            <tr style="padding:0">
                <td style="padding:0">
                    <table style="width:100%;border:0;border-spacing:0;margin:auto;">
                        <tbody>
                            <tr style="padding:0">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p class="name" style="text-align:center;">
                                        Zhipeng Zhang 张志鹏
                                    </p>
                                    <p>
                                        Dr. Zhipeng Zhang is currently a Tenure-Track Assistant Professor in <a href="https://soai.sjtu.edu.cn/">the School of Artificial Intelligence at Shanghai Jiao Tong University</a> since April 2025. After joining SJTU, he founded <strong>AutoLab</strong>. The mission of the laboratory is to automate all work that can be automated by AI. Prior to joining SJTU, he served as a Senior Researcher at <a href="https://www.kargo-bot.com/">KargBot</a> from July 2022 to March 2025, leading key AI projects in autonomous driving.
                                    </p>
                                    <p>
                                        I earned my Ph.D. from the National Laboratory of Pattern Recognition (NLPR) at the Chinese Academy of Sciences (CASIA) in 2022 under the supervision of Prof. Weiming Hu. During my doctoral studies, I completed a research internship at <a href="https://www.msra.cn/">MSRA</a> working closely with Dr. Houwen Peng, focusing on advanced computer vision and deep learning. Upon graduation, I was awarded the "Huawei Genius Young Talent".
                                    </p>
                                    <p>
                                        My recent research interests include: (1) Multimodal Perception & Video Understanding; (2) Autonomous Driving (BEV, E2E); (3) Vision-Language-Action (VLA) in Embodied AI; (4) 3D Scene Reconstruction; (5) Generative AI; 
                                    </p>

                                    <p style="text-align:center">
                                        <a href="./files/ResumeZP.pdf">CV</a> &nbsp;/&nbsp;
                                        <a href="mailto:zhipeng.zhang.cv@outlook.com">Email</a> &nbsp;/&nbsp;
                                        <a href="https://scholar.google.com/citations?user=7Ws0QHYAAAAJ&hl=en">Google Scholar</a>&nbsp;/&nbsp;
                                        <a href="https://www.xiaohongshu.com/user/profile/61310839000000000201ca67?xsec_token=YBSd9kKQNRjNhO5Nca73lJcjQy7A9GTLpfkT8-o9EFgRo=&xsec_source=app_share&xhsshare=CopyLink&appuid=61310839000000000201ca67&apptime=1753462318&share_id=88a7a37bff49461e8d38a8736a0b4168">RedBook</a>
                                    </p>
                                    
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="images/profiles/zhipeng.jpg">
                                        <img style="width:100%;max-width:100%;object-fit:cover;border-radius:50%" alt="profile photo" src="images/profiles/zhipeng.jpg" class="hoverZoomLink">
                                    </a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0;border-spacing:0;margin:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <h2>Hiring</h2>
                                    <p>
                                        <span style="color:red;">We are recruiting research assistants.</span> If you are passionate about AI and eager to contribute to cutting-edge research, don’t hesitate to contact me (see email above). Although I just joined SJTU this year, one of my Ph.D. whom I mentored for four years recently also obtained Huawei’s Genius Young Talent Program (华为天才少年). I am committed to supporting exceptional individuals like you and helping you achieve your academic and professional goals.
                                    </p>
                                    <p>
                                        <span style="color:red;">长期招聘本科远程科研实习生，需有一定AI基础。如果没基础也可以先咨询我学哪些内容，等入门后再开始。有兴趣申请2027入学的博士或者硕士提前联系做科研实习生，后序名额只在RA中选择。已经读研的慎重联系。已经保送的大四学生请不要联系实习，被坑过几次后有阴影，请谅解。 请参考上述小红书链接查看我和学生相处风格，希望有同频的学生。</span> 
                                    </p>
                                    
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0;border-spacing:0;margin:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <h2>News</h2>
                                    <ul>
                                        <li><b>[2025.09]</b> 🎉 4 papers are accepted by NeurIPS2025! (Efficient VLM/Autonomous Driving VLA/Embodied VLA, Online Embodied Segmentation, Tracking).
                                        <li><b>[2025.09]</b> 🎉 UAD is accepted by T-PAMI. WTF experience.
                                        <li><b>[2025.08]</b> 🎉 We release the code and paper of <a href="https://github.com/mystorm16/FastVGGT" target="_blank">FastVGGT</a>, 4x faster than VGGT (CVPR25's best paper) while retaining its performances.
                                        <li><b>[2025.07]</b> 🎉 MambaFusion is Selected as Highlight in ICCV2025.
                                        <li><b>[2025.06]</b> 🎉 Selected for the CCF-DiDi GAIA Scholar Program.
                                        <li><b>[2025.06]</b> 🎉 Serving as the Area Chair of WACV2026.
                                        <li><b>[2025.06]</b> 🎉 2 papers are accepted by ICCV2025 (muti-sensor fusion in autonomous driving and vision-language foundation model in Embodied AI).
                                        <li><b>[2025.05]</b> 🎉 1 paper is selected as outstanding paper in CV4Animals workshop in CVPR2025.
                                        <li><b>[2025.05]</b> 🎉 Serving as the Area Chair of BMVC2026.
                                        <li><b>[2025.05]</b> 🎉 1 paper is selected as Oral in ICME.
                                        <li><b>[2025.02]</b> 🎉 3 papers are accepted by CVPR2025 (2 corresponding/1 co-first author).
                                        <li><b>[2025.02]</b> 🎉 1 paper is accepted by ICRA2025 (first author).
                                        <li><b>[2024.09]</b> 🎉 1 paper is accepted by NeurIPS2024.
                                        <li><b>[2024.07]</b> 🎉 1 paper is accepted by ECCV2024.
                                        <li><b>[2024.07]</b> 🎉 1 paper is accepted by IJCV (corresponding author).
                                        <li><b>[2024.06]</b> 🎉 1 paper is accepted by T-PAMI (corresponding author).
                                        <li><b>[2024.02]</b> 🎉 1 paper is accepted by CVPR2024 (corresponding author).
                                        <li><b>[2023.09]</b> 🎉 1 paper is accepted as Oral by BMVC.
                                        <li><b>[2023.02]</b> 🎉 1 paper is accepted by CVPR2023 (co-first author).
                                        <li><b>[2022.09]</b> 🎉 2 papers are accepted by NeurIPS2022 (1 co-first author).
                                        <li><b>[2022.09]</b> 🎉 Our method won the runner-up of task 1-1 and 2nd runner-up of task 1-2 in 1st Learning and Mining with Noisy Labels Challenge.
                                        <li><b>[2022.03]</b> 🎉 1 paper is accepted as Oral by IJCAI2022 (co-first author).
                                        <li><b>[2021.12]</b> 🎉 1 paper is accepted by AAAI2022 (co-first author).
                                        <li><b>[2021.10]</b> 🎉 1 paper is accepted by IEEE T-IP (first author).
                                        <li><b>[2021.09]</b> 🎉 1 paper is accepted by IEEE T-IP (co-first author).
                                        <li><b>[2021.07]</b> 🎉 1 paper is accepted by ICCV2021 (first author).
                                        <li><b>[2021.02]</b> 🎉 1 paper is accepted by CVPR2021.
                                        <li><b>[2020.12]</b> 🎉 1 paper is accepted by IEEE T-IP.
                                        <li><b>[2020.09]</b> 🎉 Our method ranks at 2nd in short-term and real-time/1st in RGBT tracks of VOT Challenge.
                                        <li><b>[2020.07]</b> 🎉 1 paper is accepted by ECCV2020 (first author).
                                        <li><b>[2019.09]</b> 🎉 Our method ranks at 2nd in RGBT tracks of VOT Challenge.
                                        <li><b>[2019.02]</b> 🎉 1 paper is accepted as Oral by CVPR2019 (first author).
                                    </ul>
                                </td>
                            </tr>
                        </tbody>
                    </table>

       <!-- ================== Selected Publications ================== -->
<table style="width:100%;border:0;border-spacing:0;margin:auto;">
  <tbody>
     <!-- fastvggt -->
      <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/fastvggt.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://github.com/mystorm16/FastVGGT">
                <span class="papertitle">FastVGGT: Training-Free Acceleration of Visual Geometry Transformer</span>
            </a>
            <div>
                You Shen, <strong>Zhipeng Zhang</strong>, Yansong Qu, Liujuan Cao
            </div>
            <em>arxiv, 2025</em>
            <p><strong>FastVGGT</strong> observes strong similarity in attention maps and leverages it to design a training-free acceleration method for long-sequence 3D reconstruction, achieving up to 4× faster inference without sacrificing accuracy.</p>
        </td>
    </tr>

    <!-- mamba3vl -->
      <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/mamba3vl.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="xx">
                <span class="papertitle">Mamba-3VL: Taming State Space Model for 3D Vision Language Learning</span>
            </a>
            <div>
                Yuan Wang, Yuxin Chen, Zhongang Qi, Lijun Liu, Jile Jiao, Xuetao Feng, Yujia Liang, Ying Shan, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>
            </div>
            <em>ICCV, 2025</em>
            <p><strong>MambaFusion</strong> is a pioneering 3D-VL framework to model complex intra- and inter-modality correlations and enhance spatial relation reasoning, while guaranteeing top-tier performance, high efficiency, and generalization potential for 3D-VL tasks.</p>
        </td>
    </tr>

     <!-- mambafusion -->
      <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/mambafusion.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="xx">
                <span class="papertitle">Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection</span>
            </a>
            <div>
                Hanshi Wang, Jin Gao, Weiming Hu, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>
            </div>
            <em>ICCV, 2025</em>
            <p><strong>MambaFusion</strong> presents the first work demonstrating that a pure Mamba block can achieve efficient Dense Global Fusion, meanwhile guaranteeing top performance for camera-LiDAR multi-modal 3D object detection.</p>
        </td>
    </tr>


      <!-- CorrBEV -->
      <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/corrbev-1.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="xx">
                <span class="papertitle">CorrBEV: Multi-View 3D Object Detection by Correlation Learning with Multi-modal Prototypes</span>
            </a>
            <div>
                Ziteng Xue, Mingzhe Guo, Heng Fan, Shihui Zhang, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>
            </div>
            <em>CVPR, 2025</em>
            <p><strong>CorrBEV</strong> improves BEV detection methods in autonomous driving by introducing vision-language multimodal prototypes.</p>
        </td>
    </tr>
    
    <!-- CarGS -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/cargs-1.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://www.arxiv.org/abs/2503.00881">
                <span class="papertitle">Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization</span>
            </a>
            <div>
                You Shen, <strong>Zhipeng Zhang*</strong>, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao
            </div>
            <em>CVPR, 2025</em>
            <p><strong>CarGS</strong> simultaneously achieves promising performances in both scene reconstruction and novel view synthesis with a unified model, improving the quality of 3DGS.</p>
        </td>
    </tr>
    
    <!-- DreamTrack -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/dreamtrack2-1.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="xx">
                <span class="papertitle">DreamTrack: Dreaming the Future for Multimodal Visual Object Tracking</span>
            </a>
            <div>
                Mingzhe Guo, Weiping Tan, Wenyu Ran, Liping Jing, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>
            </div>
            <em>CVPR, 2025</em>
            <p><strong>DreamTrack</strong> shows the best performances in visual tracking by dreaming the future presentation with latent world model.</p>
        </td>
    </tr>
    
    <!-- Cycer -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/cycer-1.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://link.springer.com/article/10.1007/s11263-024-02176-7">
                <span class="papertitle">Cyclic Refiner: Object-Aware Temporal Representation Learning for Multi-View 3D Detection and Tracking</span>
            </a>
            <div>
                Mingzhe Guo, <strong>Zhipeng Zhang*<sup>&#x2709;</sup></strong>, Liping Jing, Yuan He, Ke Wang, Heng Fan
            </div>
            <em>IJCV</em>
            <p><strong>Cycer</strong> reduces false positives in BEV detection of autonomous driving by propagating results of t - 1 frame to t, which generates a mask to filter distractors in BEV representation.</p>
        </td>
    </tr>
    
    <!-- A-Teacher -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/ateacher-1.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_A-Teacher_Asymmetric_Network_for_3D_Semi-Supervised_Object_Detection_CVPR_2024_paper.pdf">
                <span class="papertitle">A-Teacher: Asymmetric Network for 3D Semi-Supervised Object Detection</span>
            </a>
            <div>
                Hanshi Wang, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>, Jin Gao, Weiming Hu
            </div>
            <em>CVPR, 2023</em>
            <p><strong>A-Teachers</strong> proposes the first online asymmetric framework for semi-supervised 3D LiDAR detection.</p>
        </td>
    </tr>
    
    <!-- UAD -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/uad.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://arxiv.org/abs/2406.17680">
                <span class="papertitle">End-to-End Autonomous Driving without Costly Modularization and 3D Manual Annotation</span>
            </a>
            <div>
                Mingzhe Guo, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>, Yuan He, Ke Wang, Liping Jing, Haibin Ling
            </div>
            <em>T-PAMI</em>
            <p><strong>UAD</strong> proposes the first work demonstrating that an unsupervised model can outperform supervised End-to- End autonomous driving method.</p>
        </td>
    </tr>
    
    <!-- VastTrack -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/vast.jpg' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/ec17a52ea4d42361ce8dde2e17dcea05-Abstract-Datasets_and_Benchmarks_Track.html">
                <span class="papertitle">VastTrack: Vast Category Visual Object Tracking</span>
            </a>
            <div>
                Liang Peng, Junyuan Gao, Xinran Liu, Weihong Li, Shaohua Dong, <strong>Zhipeng Zhang</strong>, Heng Fan, Libo Zhang
            </div>
            <em>NeurIPS, 2024</em>
            <p><strong>VAST</strong> is the largest visual tracking benchmark to date.</p>
        </td>
    </tr>
    
    <!-- AUNet -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/deepfake.jpg' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_AUNet_Learning_Relations_Between_Action_Units_for_Face_Forgery_Detection_CVPR_2023_paper.pdf">
                <span class="papertitle">AUNet: Learning Relations Between Action Units for Face Forgery Detection</span>
            </a>
            <div>
                Weiming Bai, Yufan Liu*, <strong>Zhipeng Zhang*</strong>, Bing Li, Weiming Hu
            </div>
            <em>CVPR, 2023</em>
            <p><strong>AUNet</strong> proposes the Action-Units Relation Learning framework to improve the generality of forgery (deepfake) detection.</p>
        </td>
    </tr>
    
    <!-- Ocean -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/ocean.jpg' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660766.pdf">
                <span class="papertitle">Ocean: Object-aware Anchor-free Tracking</span>
            </a>
            <div>
                <strong>Zhipeng Zhang</strong>, Houwen Peng, Jianlong Fu, Bing Li, Weiming Hu
            </div>
            <em>ECCV, 2020 (Cite 900+)</em>
            <p><strong>Ocean</strong> explores an efficient anchor-free framework to improve object tracking robustness.</p>
        </td>
    </tr>    

      <!-- SiamDW -->
      <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/siamdw.jpg' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.pdf">
                <span class="papertitle">Deeper and Wider Siamese Networks for Real-Time Visual Tracking</span>
            </a>
            <div style="white-space: nowrap;">
                <strong>Zhipeng Zhang</strong>, Houwen Peng
            </div>
            <em>CVPR, 2019 (Oral, Cite 1300+)</em>
            <p><strong>SiamDW</strong> is the first work to solve the performance degradation in the Siamese tracking framework when using a deeper network.</p>
        </td>
    </tr>  
  </tbody>
</table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <h2>Working Experience</h2>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/sjtu.jpeg" style="width:100%;text-align:center;">
                                </td>
                                <td width="75%" valign="center">
                                    <a href="https://www.shlab.org.cn/">AI School of Shanghai Jiaotong University</a>
                                    <p>Assistant Professor, 2025.04</p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/kargo2.png" style="width:100%;text-align:center;">
                                </td>
                                <td width="75%" valign="center">
                                    <a href="https://www.kargo-bot.com/">KargoBot</a>
                                    <p>Senior Researcher, 2022.07 ~ 2025.03</p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/microsoft.png" style="width:100%;text-align:center;">
                                </td>
                                <td width="75%" valign="center">
                                    <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia (MSRA)</a>
                                    <p>Research Intern, 2019.08 ~ 2020.06</p>
                                </td>
                            </tr>
                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                  <img src="images/casia.jpeg" style="width:100%;text-align:center;">
                              </td>
                              <td width="75%" valign="center">
                                  <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">NLPR, Institute of Automation, Chinese Academy of Sciences (CASIA)</a>
                                  <p>Ph.D.(直博), 2017.09 ~ 2022.07</p>
                              </td>
                          </tr>
                        </tbody>
                    </table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                        <tbody>
                            <tr>
                                <td>
                                    <h2>Miscellanea</h2>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellpadding="10">
                        <tbody>
                            <ul>
                                <li><b>Conference Area Chairs:</b> WACV, BMVC.
                                <li><b>Conference Reviewer:</b> CVPR, ICCV, ECCV, ICML, ICLR, NeurIPS, AAAI.
                                <li><b>Journal Reviewer:</b> T-PAMI, IJCV, TIP, et.al.
                                <li><b>Invited Talk:</b> SiamDW in CVPR2019 (<a href="https://www.bilibili.com/video/BV134411e7g1/?spm_id_from=333.337.search-card.all.click&vd_source=b7baadaad94318165650f979c0973100">极市平台</a>)
                                <li><b>Invited Talk:</b> Ocean in ECCV2020 (<a href="https://www.bilibili.com/video/BV1354y1e7wU/?spm_id_from=333.337.search-card.all.click&vd_source=b7baadaad94318165650f979c0973100">极市平台</a>)
                                <li> <b>Award:</b> National Scholarship for Ph.D. 
                                <li> <b>Award:</b> National Scholarship for undergraduate. 
                            </ul>
                        </tbody>
                    </table>
                </td>
            </tr>
        </tbody>
    </table>
</body>
</html>