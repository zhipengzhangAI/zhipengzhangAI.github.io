<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yanhong Zeng</title>

    <meta name="author" content="Yanhong Zeng">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yanhong Zeng æ›¾è‰³çº¢
                </p>
                <p>
                 I am currently a researcher at <a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a> in Shanghai, where I work on computer vision and machine learning.
                </p>
                <p>
                I received a PhD in Computer Science and Technology from <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a> in 2022, as a member of the joint PhD program with <a href="https://www.msra.cn/">Microsoft Research Asia (MSRA)</a>.
                I was luckily advised by <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Prof. Hongyang Chao</a>, <a href="https://www.microsoft.com/en-us/research/people/bainguo/">Baining Guo</a> and <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>. 
                </p>
                <p style="text-align:center">
                  <a href="mailto:zengyh1900@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=14LbnMIAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/zengyh1900">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/zengyh1900/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/zengyh1900/">Linkedin</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profiles/me-lol.gif"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profiles/me-lol.gif" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

<!-- ================================================================ -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>News</h2>
        
      <ul >
        <li>
          <!-- <b>[2024.03]</b>  -->
           Looking for self-motivated interns on image/video generation/editing. <a href="mailto:zengyh1900@gmail.com">Drop CV to me</a> if interested.
        </li>
        <li><b>[2024.03]</b> 
           Two papers are accepted by CVPR 2024.</li>
        <li><b>[2024.02]</b> 
           Our technology has been shipped in the animation series <a href="https://www.youtube.com/watch?v=rADkBWDsS00">"Poems of Timeless Acclaim"</a>, which is now being broadcasted on <a href="https://tv.cctv.com/2024/02/26/VIDAUw4U4rxtLHnKuKP9dFZV240226.shtml">CCTV-1</a>. </li>
        <li><b>[2024.01]</b>
           We release <a href="https://magicmaker.openxlab.org.cn/home">MagicMaker</a>, an AI platform that supports image generation, editing and animation!
        </li>
      </ul>
      </td>
    </tr>
    </tbody></table>
    
<!-- ================================================= -->          
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Research</h2>
        <p>
          I'm interested in image and video generation/editing, multi-modality learning and generation. 
        </br>
          Here are some selected publications, please check the full list from <a href="https://scholar.google.com/citations?user=14LbnMIAAAAJ&hl=en">google scholar</a>.
        <br><br>
        *Equal contribution. 
        </p>
      </td>
    </tr>
  </tbody></table>


<!-- =================================================  -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
    <tr >
      <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/make-it-vivid.png' width=80%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <span class="papertitle">Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text</span>
        </a>
        <br>
        <a href="https://junshutang.github.io/">Junshu Tang</a>,
        <strong>Yanhong Zeng</strong>,
        <a href="">Ke Fan</a>,
        <a href="">Xuheng Wang</a>,
        <a href="https://daibo.info/">Bo Dai</a>,
        <a href="https://scholar.google.com/citations?user=yd58y_0AAAAJ&hl=en">Lizhuang Ma</a>,
        <a href="https://chenkai.site/">Kai Chen</a>
        <br>
        <em>CVPR</em>, 2024
        <br>
        <a href="">paper coming soon</a>
        <!-- <a href="">video</a>/
        <a href="">arXiv</a> /
        <a href="">code</a> /
        <a href="">demo</a> -->
        <p>
        We present Make-it-Vivid, the first attempt that can create plausible and consistent texture in UV space for 3D biped cartoon characters from text input within few seconds.
        </p>
      </td>
    </tr>
    
<tr >
  <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
    <div class="one">
      <img src='images/pia.gif' width=100%>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://pi-animator.github.io/">
      <span class="papertitle">PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models</span>
    </a>
    <br>
    <a href="">Yiming Zhang*</a>,
    <a href="https://scholar.google.com/citations?user=sVYO0GYAAAAJ&hl=en">Zhening Xing*</a>,
    <strong>Yanhong Zeng</strong>,
    <a href="">Youqing Fang</a>,
    <a href="https://chenkai.site/">Kai Chen</a>
    <br>
    <em>CVPR</em>, 2024
    <br>
    <a href="https://pi-animator.github.io/">project page</a> /
    <a href="https://www.youtube.com/watch?v=A7EsYGSLpYA">video</a> /
    <a href="https://arxiv.org/abs/2312.13964">arXiv</a> /
    <a href="https://github.com/open-mmlab/PIA">code</a> /
    <a href="https://huggingface.co/spaces/Leoxing/PIA">demo</a>
    <p>
      PIA can animate any images from personalized models by text while preserving high-fidelity details and unique styles.
    </p>
  </td>
</tr>



<tr >
  <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
    <div class="one">
      <img src='images/powerpaint.png' width=100%>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://powerpaint.github.io/">
      <span class="papertitle">A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting</span>
    </a>
    <br>
    <a href="https://github.com/zhuang2002">Junhao Zhuang</a>,
    <strong>Yanhong Zeng</strong>,
    <a href="">Wenran Liu</a>,
    <a href="https://scholar.google.com/citations?hl=en&user=fYdxi2sAAAAJ">Chun Yuan</a>,
    <a href="https://chenkai.site/">Kai Chen</a>
    <br>
    <em>arxiv</em>, 2023
    <br>
    <a href="https://powerpaint.github.io/">project page</a> /
    <a href="https://www.youtube.com/watch?v=7QsiY1JQUfg">video</a> /
    <a href="https://arxiv.org/abs/2312.03594">arXiv</a> /
    <a href="https://github.com/zhuang2002/PowerPaint">code</a> /
    <a href="https://openxlab.org.cn/apps/detail/rangoliu/PowerPaint">demo</a>
    <p>
      PowerPaint is the first versatile inpainting model that achieves SOTA in text-guided and shape-guided object inpainting, object removal, outpainting, etc.
    </p>
  </td>
</tr>


<tr >
  <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
    <div class="one">
      <img src='images/aotgan.png' width=100%>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://sites.google.com/view/1900zyh/aot-gan?authuser=0">
      <span class="papertitle">Aggregated Contextual Transformations for High-Resolution Image Inpainting</span>
    </a>
    <br>
    <strong>Yanhong Zeng</strong>,
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
    <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Hongyang Chao</a>,
    <a href="https://www.microsoft.com/en-us/research/people/bainguo/">Baining Guo</a>
    <br>
    <em>TVCG</em>, 2023
    <br>
    <a href="https://sites.google.com/view/1900zyh/aot-gan?authuser=0">project page</a> / 
    <a href="https://arxiv.org/abs/2104.01431">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=lYwFv0lPgOI">video 1</a> /
    <a href="https://www.youtube.com/watch?v=eCE8v10g7-E">video 2</a> /
    <a href="https://github.com/researchmm/AOT-GAN-for-Inpainting">code</a> 
    <p>
      In AOT-GAN, we propose aggregated contextual transformations and a novel mask-guided GAN training strategy for high-resolution image inpaining.
    </p>
  </td>
</tr>


<tr> 
  <td style="padding:20px;width:25%;vertical-align:middle">
    <a href="images/hdvila.png"><img src="images/hdvila.png" alt="hdvila" width="100%"></a>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2111.10337">
      <papertitle>Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions</papertitle>
    </a>
    <br>
    <a href="https://hellwayxue.github.io/">Hongwei Xue</a>, 
    <a href="https://tiankaihang.github.io/">Tiankai Hang*</a>,
    <strong>Yanhong Zeng*</strong>,
    <a href="https://scholar.google.com/citations?user=DuSxNqgAAAAJ">Yuchong Sun*</a>, 
    <a href="https://www.microsoft.com/en-us/research/people/libei/">Bei Liu</a>, 
    <a href="https://hyang0511.github.io/">Huan Yang</a>,
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
    <a href="https://www.microsoft.com/en-us/research/people/bainguo/">Baining Guo</a>
    <br>
    <em>CVPR</em>, 2022
    <br>
    <a href="https://arxiv.org/abs/2111.10337">arXiv</a> / 
    <a href="https://www.youtube.com/watch?v=DyN8ypuxX08">video</a> /
    <a href="https://github.com/microsoft/XPretrain">code</a> 
    <p>We collect a large dataset which is the first high-resolution dataset including 371.5k hours of 720p videos and the most diversified dataset covering 15 popular YouTube categories. </p>
  </td>
</tr>


<tr> 
  <td style="padding:20px;width:25%;vertical-align:middle">
    <a href="images/hdvila.png"><img src="images/tokengan.jpg" width="100%"></a>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2111.03481">
      <papertitle>Improving Visual Quality of Image Synthesis by A Token-based Generator with Transformers</papertitle>
    </a>
    <br>
    <strong>Yanhong Zeng</strong>,
    <a href="https://hyang0511.github.io/">Huan Yang</a>, 
    <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Hongyang Chao</a>, 
    <a href="https://scholar.google.com/citations?user=ZFKybdUAAAAJ&hl=en">Jianbo Wang</a>,
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>
    <br>
    <em>NeurIPS</em>, 2021
    <br>
    <a href="https://arxiv.org/abs/2111.03481">arXiv</a> 
    <p>We propose a token-based generator with Transformers for image synthesis. We present a new perspective by viewing this task as visual token generation, controlled by style tokens.</p>
  </td>
</tr>


<tr> 
  <td style="padding:20px;width:25%;vertical-align:middle">
    <a href="images/sttn.jpg"><img src="images/sttn.jpg" width="100%"></a>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://sites.google.com/view/1900zyh/sttn">
      <papertitle>Learning Joint Spatial-Temporal Transformations for Video Inpainting</papertitle>
    </a>
    <br>
    <strong>Yanhong Zeng</strong>,
    <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Hongyang Chao</a>, 
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>
    <br>
    <em>ECCV</em>, 2020
    <br>
    <a href="https://sites.google.com/view/1900zyh/sttn">project page</a> / 
    <a href="https://arxiv.org/abs/2007.10247">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=X_jD4vsuOJc">video 1 </a> /
    <a href="https://www.youtube.com/watch?v=tgiWGdr1SnE">more results</a> /
    <a href="https://github.com/researchmm/STTN">code</a>
    <p>We propose STTN, the first transformer-based model for high-quality image inpainting, setting a new state-of-the-art performance.</p>
  </td>
</tr>


<tr> 
  <td style="padding:20px;width:25%;vertical-align:middle">
    <a href="images/pennet.gif"><img src="images/pennet.gif" width="100%"></a>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://sites.google.com/view/1900zyh/pen-net?authuser=0">
      <papertitle>Learning Pyramid Context-Encoder Network for High-Quality Image Inpainting</papertitle>
    </a>
    <br>
    <strong>Yanhong Zeng</strong>,
    <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Hongyang Chao</a>, 
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
    <a href="https://www.microsoft.com/en-us/research/people/bainguo/">Baining Guo</a>
    <br>
    <em>CVPR</em>, 2019
    <br>
    <a href="https://sites.google.com/view/1900zyh/pen-net?authuser=0">project page</a> / 
    <a href="https://arxiv.org/abs/1904.07475">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=Tx2lBGybfNA">video</a> /
    <a href="https://github.com/researchmm/PEN-Net-for-Inpainting">code</a>
    <p>We propose PEN-Net, the first work that is able to conduct both semantic and texture inpainting. To achieve this, we propose cross-layer attention transfer and pyramid filling strategy.</p>
  </td>
</tr>



<tr> 
  <td style="padding:20px;width:25%;vertical-align:middle">
    <a href="images/3dhumanbody.png"><img src="images/3dhumanbody.png" width="100%"></a>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://sites.google.com/view/1900zyh/3dhumanbody?authuser=0">
      <papertitle>3D Human Body Reshaping with Anthropometric Modeling</papertitle>
    </a>
    <br>
    <strong>Yanhong Zeng</strong>,
    <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en">Hongyang Chao</a>, 
    <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>
    <br>
    <em>CVPR</em>, 2019
    <br>
    <a href="https://sites.google.com/view/1900zyh/3dhumanbody?authuser=0">project page</a> / 
    <a href="https://arxiv.org/abs/2104.01762">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=s0GvkER-Y24">video</a> /
    <a href="https://github.com/zengyh1900/3D-Human-Body-Shape">code</a>
    <p>We design a 3D human body reshaping system. It can take as input user's anthropometric measurements (e.g., height and weight) and generate a 3D human shape for the user.</p>
  </td>
</tr>


</tbody></table>



<!-- ====================== Projects =========================== -->   
          
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
      <tr>
        <td>
          <h2>Projects</h2>
        </td>
      </tr>
    </tbody></table>
    <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/magicmaker.png" style="width:100%;text-align: center;" ></td>
        <td width="75%" valign="center">
          <a href="https://magicmaker.openxlab.org.cn/home">MagicMaker</a>
          <p>Project Owner, 2023.04 ~ 2024.01</p>
          MagicMaker is a user-friendly AI platform that enables seamless image generation, editing, and animation. It empowers users to transform their imagination into captivating cinema and animations with ease.
          <br>
        </td>
      </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmagic.png" width="200px"></td>
              <td width="75%" valign="center">
                <a href="https://github.com/open-mmlab/mmagic">OpenMMLab/MMagic</a>
                <p>Lead Core Maintainer, 2022.08 ~ 2023.08</p>
                OpenMMLab Multimodal Advanced, Generative, and Intelligent Creation Toolbox. Unlock the magic ðŸª„: Generative-AI (AIGC), easy-to-use APIs, awsome model zoo, diffusion models, for text-to-image generation, image/video restoration/enhancement, etc.
                <br>
              </td>
            </tr>

        </tbody></table>
      


<!-- =================== working ============================== -->   
          
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td>
      <h2>Working Experience</h2>
    </td>
  </tr>
</tbody></table>
<table width="100%" align="center" border="0" cellpadding="20"><tbody>
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/shailab.jpg" style="width:100%;text-align: center;" ></td>
    <td width="75%" valign="center">
      <a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a>
      <p>Researcher, 2022.08 ~ Present</p>
      <br>
    </td>
  </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/microsoft.png"  style="width:100%;text-align: center;"></td>
          <td width="75%" valign="center">
            <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia (MSRA)</a>
            <p>Research Intern, 2018.06 ~ 2021.12</p>
            <p>Research Intern, 2016.06 ~ 2017.06</p>
            <br>
          </td>
        </tr>

    </tbody></table>




<!-- ===================== MISC ============================ -->   
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
      <tr>
        <td>
          <h2>Miscellanea</h2>
        </td>
      </tr>
    </tbody></table>
    <table width="100%" align="center" border="0" cellpadding="10"><tbody>
      <ul>
        <li><b>Conference Reviewer:</b>
          CVPR, ECCV, ICCV, SIGGRAPH, ICML, ICLR, NeurIPS, AAAI. 
        </li>
        <li><b>Journal Reviewer:</b>
           TIP, TVCG, TCSVT, PR. 
        </li>
        <li><b> Conference Tutorial (ICCV 2023):</b>
          <a href="https://www.bilibili.com/video/BV1P94y187HE/?spm_id_from=333.999.0.0&vd_source=4a2f786f73e4f84c86d5a2e1db9d6e97">
            MMagic: Multimodal Advanced, Generative and Intelligent Creation
            </a>
          </li>
        <li><b> Conference Tutorial (CVPR 2023):</b>
          <a href="https://www.bilibili.com/video/BV1nX4y1q7DF/?spm_id_from=333.999.0.0&vd_source=4a2f786f73e4f84c86d5a2e1db9d6e97">Learning to Generate, Edit, and Enhance Images and Videos with MMagic</a>
        </li>
        <li><b>Invited Talk:</b>
          Towards High-Quality Image Inpainting (<a href="https://www.bilibili.com/video/BV1YJ411E7et/?spm_id_from=333.999.0.0">Microsoft China Video Center on Bilibili Live 2019</a>)
        </li>
        <li>  <b>Award:</b>
          ICML 2022 Outstanding Reviewer. </li>
        <li> <b>Award:</b>
           Scholarship in 2021 (Top 1% in SYSU). </li>
        <li>  <b>Award:</b>
          Outstanding Undergraduate Thesis in 2017.</li>
        <li>  <b>Award:</b>
          Outstanding Undergraduate in 2017.</li>
        <li>  <b>Award:</b>
          National Scholarship in 2016 (Top 1% in SYSU).</li>
        <li>  <b>Award:</b>
          First Prize Excellence Scholarship in 2013, 2014, 2015.</li>
        </ul>      
      
  </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
