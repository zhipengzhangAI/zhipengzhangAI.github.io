<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Zhipeng Zhang</title>
    <meta name="author" content="Zhipeng Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
    <table style="width:100%;max-width:800px;border:0;border-spacing:0;margin:auto;">
        <tbody>
            <tr style="padding:0">
                <td style="padding:0">
                    <table style="width:100%;border:0;border-spacing:0;margin:auto;">
                        <tbody>
                            <tr style="padding:0">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p class="name" style="text-align:center;">
                                        Zhipeng Zhang å¼ å¿—é¹
                                    </p>
                                    <p>
                                        Dr. Zhipeng Zhang is currently a Tenure-Track Assistant Professor in <a href="https://soai.sjtu.edu.cn/">the School of Artificial Intelligence at Shanghai Jiao Tong University</a> since April 2025. Prior to joining SJTU, he served as a Senior Researcher at <a href="https://www.kargo-bot.com/">KargBot</a> from July 2022 to March 2025, leading key AI projects in autonomous driving.
                                    </p>
                                    <p>
                                        I earned my Ph.D. from the National Laboratory of Pattern Recognition (NLPR) at the Chinese Academy of Sciences (CASIA) in 2022 under the supervision of Prof. Weiming Hu. During my doctoral studies, I completed a research internship at <a href="https://www.msra.cn/">MSRA</a> working closely with Dr. Houwen Peng, focusing on advanced computer vision and deep learning. Upon graduation, I was awarded the "Huawei Genius Young Talent".
                                    </p>
                                    <p>
                                        My recent research interests include: (1) Multimodal Perception & Video Understanding; (2) Autonomous Driving (BEV, E2E); (3) Vision-Language-Action (VLA) in Embodied AI; (4) 3D Scene Reconstruction; (5) Generative AI; 
                                    </p>

                                    <p style="text-align:center">
                                        <a href="./files/ResumeZP.pdf">CV</a> &nbsp;/&nbsp;
                                        <a href="mailto:zhipeng.zhang.cv@outlook.com">Email</a> &nbsp;/&nbsp;
                                        <a href="https://scholar.google.com/citations?user=7Ws0QHYAAAAJ&hl=en">Google Scholar</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="images/profiles/zhipeng.jpg">
                                        <img style="width:100%;max-width:100%;object-fit:cover;border-radius:50%" alt="profile photo" src="images/profiles/zhipeng.jpg" class="hoverZoomLink">
                                    </a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0;border-spacing:0;margin:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <h2>Hiring</h2>
                                    <p>
                                        <span style="color:red;">We are recruiting research assistants.</span> If you are passionate about AI and eager to contribute to cutting-edge research, donâ€™t hesitate to contact me (see email above). Although I just joined SJTU this year, one of my Ph.D. whom I mentored for four years recently also obtained Huaweiâ€™s Genius Young Talent Program (åä¸ºå¤©æ‰å°‘å¹´). I am committed to supporting exceptional individuals like you and helping you achieve your academic and professional goals.
                                    </p>
                                    <p>
                                        <span style="color:red;">é•¿æœŸæ‹›è˜è¿œç¨‹ç§‘ç ”å®ä¹ ç”Ÿï¼Œéœ€æœ‰ä¸€å®šAIåŸºç¡€ã€‚å¦‚æœæ²¡åŸºç¡€ä¹Ÿå¯ä»¥å…ˆå’¨è¯¢æˆ‘å­¦å“ªäº›å†…å®¹ï¼Œç­‰å…¥é—¨åå†å¼€å§‹ã€‚2026Fallå…¥å­¦çš„åšå£«å’Œç¡•å£«åé¢å·²ç»ç”¨å®Œï¼Œæ— éœ€å†å‘é‚®ä»¶å¥—ç“·ã€‚è¯·æœ‰å…´è¶£ç”³è¯·2027å…¥å­¦çš„åšå£«æˆ–è€…ç¡•å£«ï¼ˆä¸»è¦æ˜¯ç°å¤§äºŒçš„åŒå­¦ï¼‰æå‰è”ç³»åšç§‘ç ”å®ä¹ ç”Ÿï¼Œååºåé¢åªåœ¨RAä¸­é€‰æ‹©ã€‚</span> 
                                    </p>
                                    
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0;border-spacing:0;margin:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <h2>News</h2>
                                    <ul>
                                        <li><b>[2025.05]</b> ğŸ‰ Serving as the AC of BMVC.
                                        <li><b>[2025.05]</b> ğŸ‰ 1 paper is selected as Oral in ICME.
                                        <li><b>[2025.02]</b> ğŸ‰ 3 papers are accepted by CVPR2025 (2 corresponding/1 co-first author).
                                        <li><b>[2025.02]</b> ğŸ‰ 1 paper is accepted by ICRA2025 (first author).
                                        <li><b>[2024.09]</b> ğŸ‰ 1 paper is accepted by NeurIPS2024.
                                        <li><b>[2024.07]</b> ğŸ‰ 1 paper is accepted by ECCV2024.
                                        <li><b>[2024.07]</b> ğŸ‰ 1 paper is accepted by IJCV (corresponding author).
                                        <li><b>[2024.06]</b> ğŸ‰ 1 paper is accepted by T-PAMI (corresponding author).
                                        <li><b>[2024.02]</b> ğŸ‰ 1 paper is accepted by CVPR2024 (corresponding author).
                                        <li><b>[2023.09]</b> ğŸ‰ 1 paper is accepted as Oral by BMVC.
                                        <li><b>[2023.02]</b> ğŸ‰ 1 paper is accepted by CVPR2023 (co-first author).
                                        <li><b>[2022.09]</b> ğŸ‰ 2 papers are accepted by NeurIPS2022 (1 co-first author).
                                        <li><b>[2022.09]</b> ğŸ‰ Our method won the runner-up of task 1-1 and 2nd runner-up of task 1-2 in 1st Learning and Mining with Noisy Labels Challenge.
                                        <li><b>[2022.03]</b> ğŸ‰ 1 paper is accepted as Oral by IJCAI2022 (co-first author).
                                        <li><b>[2021.12]</b> ğŸ‰ 1 paper is accepted by AAAI2022 (co-first author).
                                        <li><b>[2021.10]</b> ğŸ‰ 1 paper is accepted by IEEE T-IP (first author).
                                        <li><b>[2021.09]</b> ğŸ‰ 1 paper is accepted by IEEE T-IP (co-first author).
                                        <li><b>[2021.07]</b> ğŸ‰ 1 paper is accepted by ICCV2021 (first author).
                                        <li><b>[2021.02]</b> ğŸ‰ 1 paper is accepted by CVPR2021.
                                        <li><b>[2020.12]</b> ğŸ‰ 1 paper is accepted by IEEE T-IP.
                                        <li><b>[2020.09]</b> ğŸ‰ Our method ranks at 2nd in short-term and real-time/1st in RGBT tracks of VOT Challenge.
                                        <li><b>[2020.07]</b> ğŸ‰ 1 paper is accepted by ECCV2020 (first author).
                                        <li><b>[2019.09]</b> ğŸ‰ Our method ranks at 2nd in RGBT tracks of VOT Challenge.
                                        <li><b>[2019.02]</b> ğŸ‰ 1 paper is accepted as Oral by CVP2019 (first author).
                                    </ul>
                                </td>
                            </tr>
                        </tbody>
                    </table>

       <!-- ================== Selected Publications ================== -->
<table style="width:100%;border:0;border-spacing:0;margin:auto;">
  <tbody>
      <!-- CorrBEV -->
      <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/corrbev-1.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="xx">
                <span class="papertitle">CorrBEV: Multi-View 3D Object Detection by Correlation Learning with Multi-modal Prototypes</span>
            </a>
            <div>
                Ziteng Xue, Mingzhe Guo, Heng Fan, Shihui Zhang, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>
            </div>
            <em>CVPR, 2025</em>
            <p><strong>CorrBEV</strong> improves BEV detection methods in autonomous driving by introducing vision-language multimodal prototypes.</p>
        </td>
    </tr>
    
    <!-- CarGS -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/cargs-1.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://www.arxiv.org/abs/2503.00881">
                <span class="papertitle">Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization</span>
            </a>
            <div>
                You Shen, <strong>Zhipeng Zhang*</strong>, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao
            </div>
            <em>CVPR, 2025</em>
            <p><strong>CarGS</strong> simultaneously achieves promising performances in both scene reconstruction and novel view synthesis with a unified model, improving the quality of 3DGS.</p>
        </td>
    </tr>
    
    <!-- DreamTrack -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/dreamtrack2-1.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="xx">
                <span class="papertitle">DreamTrack: Dreaming the Future for Multimodal Visual Object Tracking</span>
            </a>
            <div>
                Mingzhe Guo, Weiping Tan, Wenyu Ran, Liping Jing, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>
            </div>
            <em>CVPR, 2025</em>
            <p><strong>DreamTrack</strong> shows the best performances in visual tracking by dreaming the future presentation with latent world model.</p>
        </td>
    </tr>
    
    <!-- Cycer -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/cycer-1.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://link.springer.com/article/10.1007/s11263-024-02176-7">
                <span class="papertitle">Cyclic Refiner: Object-Aware Temporal Representation Learning for Multi-View 3D Detection and Tracking</span>
            </a>
            <div>
                Mingzhe Guo, <strong>Zhipeng Zhang*<sup>&#x2709;</sup></strong>, Liping Jing, Yuan He, Ke Wang, Heng Fan
            </div>
            <em>IJCV</em>
            <p><strong>Cycer</strong> reduces false positives in BEV detection of autonomous driving by propagating results of t - 1 frame to t, which generates a mask to filter distractors in BEV representation.</p>
        </td>
    </tr>
    
    <!-- A-Teacher -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/ateacher-1.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_A-Teacher_Asymmetric_Network_for_3D_Semi-Supervised_Object_Detection_CVPR_2024_paper.pdf">
                <span class="papertitle">A-Teacher: Asymmetric Network for 3D Semi-Supervised Object Detection</span>
            </a>
            <div>
                Hanshi Wang, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>, Jin Gao, Weiming Hu
            </div>
            <em>CVPR, 2023</em>
            <p><strong>A-Teachers</strong> proposes the first online asymmetric framework for semi-supervised 3D LiDAR detection.</p>
        </td>
    </tr>
    
    <!-- UAD -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/uad.png' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://arxiv.org/abs/2406.17680">
                <span class="papertitle">End-to-End Autonomous Driving without Costly Modularization and 3D Manual Annotation</span>
            </a>
            <div>
                Mingzhe Guo, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>, Yuan He, Ke Wang, Liping Jing, Haibin Ling
            </div>
            <em>Arxiv</em>
            <p><strong>UAD</strong> proposes the first work demonstrating that an unsupervised model can outperform supervised End-to- End autonomous driving method.</p>
        </td>
    </tr>
    
    <!-- VastTrack -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/vast.jpg' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/ec17a52ea4d42361ce8dde2e17dcea05-Abstract-Datasets_and_Benchmarks_Track.html">
                <span class="papertitle">VastTrack: Vast Category Visual Object Tracking</span>
            </a>
            <div>
                Liang Peng, Junyuan Gao, Xinran Liu, Weihong Li, Shaohua Dong, <strong>Zhipeng Zhang</strong>, Heng Fan, Libo Zhang
            </div>
            <em>NeurIPS, 2024</em>
            <p><strong>VAST</strong> is the largest visual tracking benchmark to date.</p>
        </td>
    </tr>
    
    <!-- AUNet -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/deepfake.jpg' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_AUNet_Learning_Relations_Between_Action_Units_for_Face_Forgery_Detection_CVPR_2023_paper.pdf">
                <span class="papertitle">AUNet: Learning Relations Between Action Units for Face Forgery Detection</span>
            </a>
            <div>
                Weiming Bai, Yufan Liu*, <strong>Zhipeng Zhang*</strong>, Bing Li, Weiming Hu
            </div>
            <em>CVPR, 2023</em>
            <p><strong>AUNet</strong> proposes the Action-Units Relation Learning framework to improve the generality of forgery (deepfake) detection.</p>
        </td>
    </tr>
    
    <!-- Ocean -->
    <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/ocean.jpg' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; max-width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660766.pdf">
                <span class="papertitle">Ocean: Object-aware Anchor-free Tracking</span>
            </a>
            <div>
                <strong>Zhipeng Zhang</strong>, Houwen Peng, Jianlong Fu, Bing Li, Weiming Hu
            </div>
            <em>ECCV, 2020 (Cite 900+)</em>
            <p><strong>Ocean</strong> explores an efficient anchor-free framework to improve object tracking robustness.</p>
        </td>
    </tr>    

      <!-- SiamDW -->
      <tr style="display: flex; align-items: stretch;">
        <td style="padding: 20px; width: 25%; display: flex; align-items: center; justify-content: center;">
            <div style="width: 100%;">
                <img src='images/siamdw.jpg' style="width: 100%; height: auto; object-fit: contain;">
            </div>
        </td>
        <td style="padding: 20px; width: 75%; display: flex; flex-direction: column; justify-content: center;">
            <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.pdf">
                <span class="papertitle">Deeper and Wider Siamese Networks for Real-Time Visual Tracking</span>
            </a>
            <div style="white-space: nowrap;">
                <strong>Zhipeng Zhang</strong>, Houwen Peng
            </div>
            <em>CVPR, 2019 (Oral, Cite 1200+)</em>
            <p><strong>SiamDW</strong> is the first work to solve the performance degradation in the Siamese tracking framework when using a deeper network.</p>
        </td>
    </tr>  
  </tbody>
</table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <h2>Working Experience</h2>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/sjtu.jpeg" style="width:100%;text-align:center;">
                                </td>
                                <td width="75%" valign="center">
                                    <a href="https://www.shlab.org.cn/">AI School of Shanghai Jiaotong University</a>
                                    <p>Assistant Professor, 2025.04</p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/kargo2.png" style="width:100%;text-align:center;">
                                </td>
                                <td width="75%" valign="center">
                                    <a href="https://www.kargo-bot.com/">KargoBot</a>
                                    <p>Senior Researcher, 2022.07 ~ 2025.03</p>
                                </td>
                            </tr>
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/microsoft.png" style="width:100%;text-align:center;">
                                </td>
                                <td width="75%" valign="center">
                                    <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia (MSRA)</a>
                                    <p>Research Intern, 2019.08 ~ 2020.06</p>
                                </td>
                            </tr>
                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                  <img src="images/casia.jpeg" style="width:100%;text-align:center;">
                              </td>
                              <td width="75%" valign="center">
                                  <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">NLPR, Institute of Automation, Chinese Academy of Sciences (CASIA)</a>
                                  <p>Ph.D.(ç›´åš), 2017.09 ~ 2022.07</p>
                              </td>
                          </tr>
                        </tbody>
                    </table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                        <tbody>
                            <tr>
                                <td>
                                    <h2>Miscellanea</h2>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellpadding="10">
                        <tbody>
                            <ul>
                                <li><b>Conference Reviewer:</b> CVPR, ICCV, ECCV, ICML, ICLR, NeurIPS, AAAI.
                                <li><b>Journal Reviewer:</b> T-PAMI, IJCV, TIP, et.al.
                                <li><b>Invited Talk:</b> SiamDW in CVPR2019 (<a href="https://www.bilibili.com/video/BV134411e7g1/?spm_id_from=333.337.search-card.all.click&vd_source=b7baadaad94318165650f979c0973100">æå¸‚å¹³å°</a>)
                                <li><b>Invited Talk:</b> Ocean in ECCV2020 (<a href="https://www.bilibili.com/video/BV1354y1e7wU/?spm_id_from=333.337.search-card.all.click&vd_source=b7baadaad94318165650f979c0973100">æå¸‚å¹³å°</a>)
                                <li> <b>Award:</b> National Scholarship for Ph.D. 
                                <li> <b>Award:</b> National Scholarship for undergraduate. 
                            </ul>
                        </tbody>
                    </table>
                </td>
            </tr>
        </tbody>
    </table>
</body>
</html>