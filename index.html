<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zhipeng Zhang</title>

    <meta name="author" content="Zhipeng Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortHTML Previewcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zhipeng Zhang Âº†ÂøóÈπè
                </p>
                <p>
        
                  Dr. Zhipeng Zhang is currently a Tenure-Track Assistant Professor in <a href="https://soai.sjtu.edu.cn/">the School of Artificial Intelligence at Shanghai Jiao Tong University</a> since April 2025. Prior to joining SJTU, he served as a Senior Researcher at <a href="https://www.kargo-bot.com/">KargBot</a> from July 2022 to March 2025, leading key AI projects in autonomous driving.
                </p>
                <p>
                He earned his Ph.D. from the National Laboratory of Pattern Recognition (NLPR) at the Chinese Academy of Sciences (CASIA) in 2022 under the supervision of Prof. Weiming Hu. During his doctoral studies, he completed a research internship at <a href="https://www.msra.cn/">MSRA</a> under the mentorship of Dr. Houwen Peng, focusing on advanced computer vision and deep learning.
                  Upon graduation, he was awarded the "Huawei Genius Young Talent".
                </p>
                <p>
                    His recent research interests include: (1) Multimodal Perception & Video Understanding; (2) Autonomous Driving (BEV, E2E); (3) Vision-Language-Action (VLA) in Embodied AI; (4) 3D Scene Reconstruction; (5) Generative AI; (6) Model Quantization for Edge Devices.
                </p>

                <p style="text-align:center">
                  <!-- <a href="https://zengyh1900.github.io/zengyh1900_cv.pdf">CV</a> &nbsp;/&nbsp; -->
                  <a href="mailto:zhipeng.zhang.cv@outlook.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="https://scholar.google.com/citations?user=7Ws0QHYAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=7Ws0QHYAAAAJ&hl=en">Google Scholar</a>
                  <!-- <a href="https://twitter.com/zengyh1900">Twitter</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://github.com/zengyh1900/">Github</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://www.linkedin.com/in/zengyh1900/">Linkedin</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <!-- <a href="images/profiles/zhipeng.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profiles/zhipeng.jpg" class="hoverZoomLink"></a> -->
                <a href="images/profiles/zhipeng.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profiles/zhipeng.jpg" class="hoverZoomLink"></a>
                <!-- <a href="images/profiles/me-angry.gif"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profiles/me-angry.gif" class="hoverZoomLink"></a> -->
                <!-- <p style="text-align: center;">status: struggle with ddl</p> -->
              </td>
            </tr>
          </tbody></table>

<!-- ================================================================ -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>News</h2>
      <ul >
        <li><b>[2025.02]</b>
          üéâ 3 papers are accepted as by CVPR2025 (2 corresponding/1 co-first author).
        <li><b>[2025.02]</b>
          üéâ 1 paper is accepted as by ICRA2025 (first author).
        <li><b>[2024.09]</b>
          üéâ 1 paper is accepted by NeurIPS2024.
        <li><b>[2024.07]</b>
          üéâ 1 paper is accepted by ECCV2024.
        <li><b>[2024.07]</b>
          üéâ 1 papers is accepted by IJCV (corresponding author).
        <li><b>[2024.06]</b>
          üéâ 1 paper is accepted by T-PAMI (corresponding author).
        <li><b>[2024.02]</b>
          üéâ 1 paper is accepted by CVPR2024 (corresponding author).
        <li><b>[2023.09]</b>
          üéâ 1 paper is accepted as Oral by BMVC.
        <li><b>[2023.02]</b>
          üéâ 1 paper is accepted by CVPR2023 (co-first author).
        <li><b>[2022.09]</b>
          üéâ 2 papers are accepted by NeurIPS2022 (1 co-first author).
        <li><b>[2022.09]</b>
          üéâ Our method won the runner-up of task 1-1 and 2nd runner-up of task 1-2 in 1st Learning and Mining with Noisy Labels Challenge.
        <li><b>[2022.03]</b>
          üéâ 1 paper is accepted as Oral by IJCAI2022 (co-first author).
        <li><b>[2021.12]</b>
          üéâ 1 paper is accepted by AAAI2022 (co-first author).
        <li><b>[2021.10]</b>
          üéâ 1 paper is accepted by IEEE T-IP (first author).
        <li><b>[2021.09]</b>
          üéâ 1 paper is accepted by IEEE T-IP (co-first author).
        <li><b>[2021.07]</b>
          üéâ 1 paper is accepted by ICCV2021 (first author).
        <li><b>[2021.02]</b>
          üéâ 1 paper is accepted by CVPR2021.
        <li><b>[2020.12]</b>
          üéâ 1 paper is accepted by IEEE T-IP.
        <li><b>[2020.09]</b>
          üéâ Our method ranks at 2nd in short-term and real-time/1st in RGBT tracks of VOT Challenge.
        <li><b>[2020.07]</b>
          üéâ 1 paper is accepted by ECCV2020 (first author).
        <li><b>[2019.09]</b>
            üéâ Our method ranks at 2nd in RGBT tracks of VOT Challenge.
        <li><b>[2019.02]</b>
          üéâ 1 paper is accepted as Oral by CVP2019 (first author).
             
  

      </ul>
      </td>
    </tr>
    </tbody></table>

<!-- ================================================= -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Selected Publications</h2>
        <!-- <p>
        *Equal contribution. &diams; Corresponding author. <br>
        </p> -->
      </td>
    </tr>
  </tbody></table>


<!-- ====================corrbev=============================  -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



    <tr >
        <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
          <div class="one">
            <img src='images/corrbev-1.png' width=100%>
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="xx">
            <span class="papertitle">
              CorrBEV: Multi-View 3D Object Detection by Correlation Learning with
              Multi-modal Prototypes
            </span>
          </a>
          <br>
          <!-- <a href="https://jianzongwu.github.io/">Jianzong Wu</a>,
          <a href="">Chao Tang</a>,
          <a href="https://wangjingbo1219.github.io/">Jingbo Wang</a>,
          <strong>Zhipeng Zhang</strong>,
          <a href="https://lxtgh.github.io/">Xiangtai Li</a>,
          <a href="https://scholar.google.com/citations?user=T4gqdPkAAAAJ">Yunhai Tong</a> -->
          Ziteng Xue, Mingzhe Guo, Heng Fan, Shihui Zhang, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>  
   
          <br>
          <em>CVPR</em>, 2025
          <br>
          <!-- <a href="https://jianzongwu.github.io/projects/diffsensei/">project page</a> /
          <a href="https://arxiv.org/abs/2412.07589">arXiv</a> /
          <a href="https://huggingface.co/datasets/jianzongwu/MangaZero">dataset</a> /
          <a href="https://jianzongwu.github.io/projects/diffsensei/static/pdfs/nobel_prize.pdf">demo</a> /
          <a href="https://github.com/jianzongwu/DiffSensei">code</a>
          <img src="https://img.shields.io/github/stars/jianzongwu/DiffSensei?style=social"> -->
          <p>
            <strong>CorrBEV</strong> improves BEV detection methods in autonomous driving by introducing vision-language multimodal prototypes.
          </p>
        </td>
      </tr>
    
    <!-- cargs -->
    <tr >
      <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/cargs-1.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.arxiv.org/abs/2503.00881">
          <span class="papertitle">
            Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization
          </span>
        </a>
        <br>
        You Shen1, <strong>Zhipeng Zhang*</strong>, Xinyang Li1, Yansong Qu1, Yu Lin, Shengchuan Zhang, Liujuan Cao
  
        <br>
        <em>CVPR</em>, 2025
        <br>
        <p>
          <strong>CarGS</strong> simultaneously achieves promising performances in both scene reconstruction and novel view synthesis with a unified model, improving the quality of 3DGS.
        </p>
      </td>
    </tr>
    
    <!-- dream track-->
    <tr >
      <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/dreamtrack2-1.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="xx">
          <span class="papertitle">
            DreamTrack: Dreaming the Future for Multimodal Visual Object Tracking
          </span>
        </a>
        <br>
        Mingzhe Guo, Weiping Tan, Wenyu Ran, Liping Jing, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>
        <br>
        <em>CVPR, 2025</em>
        <br>
        <p>
          <strong>DreamTrack</strong> shows the best performances in visual tracking by dreaming the future presentation with latent world model.
        </p>
      </td>
    </tr>

     <!-- cycer-->
     <tr >
      <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/cycer-1.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://link.springer.com/article/10.1007/s11263-024-02176-7">
          <span class="papertitle">
            Cyclic Refiner: Object-Aware Temporal Representation Learning for Multi-View 3D Detection and Tracking
          </span>
        </a>
        <br>
        Mingzhe Guo, <strong>Zhipeng Zhang*<sup>&#x2709;</sup></strong>, Liping Jing, Yuan He, Ke Wang, Heng Fan
  
        <br>
        <em>IJCV</em>
        <br>
        <p>
          <strong>Cycer</strong> reduces false positives in BEV detection of autonomous driving by propagating results of t - 1 frame to t, which generates a mask to filter distractors in BEV representation.
        </p>
      </td>
    </tr>

    <!-- ateacher -->
    <tr >
      <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/ateacher-1.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_A-Teacher_Asymmetric_Network_for_3D_Semi-Supervised_Object_Detection_CVPR_2024_paper.pdf">
          <span class="papertitle">
            A-Teacher: Asymmetric Network for 3D Semi-Supervised Object Detection
          </span>
        </a>
        <br>
        Hanshi Wang, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>, Jin Gao, Weiming Hu  
        <br>
        <em>CVPR, 2023</em>
        <br>
        <p>
          <strong>A-Teachers</strong> proposes the first online asymmetric framework for semi-supervised 3D LiDAR detection.
        </p>
      </td>
    </tr>
    <!-- uad -->
    <tr >
      <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/uad.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2406.17680">
          <span class="papertitle">
            End-to-End Autonomous Driving without Costly Modularization and 3D Manual Annotation
        </a>
        <br>
        Mingzhe Guo, <strong>Zhipeng Zhang<sup>&#x2709;</sup></strong>, Yuan He, Ke Wang, Liping Jing, Haibin Ling
        <br>
        <em>Arxiv</em>
        <br>
        <p>
          <strong>UAD</strong> proposes the first work demonstrating that an unsupervised model can outperform supervised End-to- End autonomous driving method.
      </td>
    </tr>

    <!-- vast -->
    <tr >
      <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
        <div class="one">
          <img src='images/vast.jpg' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/ec17a52ea4d42361ce8dde2e17dcea05-Abstract-Datasets_and_Benchmarks_Track.html">
          <span class="papertitle">
            VastTrack: Vast Category Visual Object Tracking
          </a>
        <br>
        Liang Peng, Junyuan Gao, Xinran Liu, Weihong Li, Shaohua Dong, <strong>Zhipeng Zhang</strong>, Heng Fan, Libo Zhang
        <br>
        <em>NeurIPS, 2024</em>
        <br>
        <p>
          <strong>VAST</strong> is the largest visual tracking benchmark to date.
      </td>
    </tr>

  <!-- deepfake -->
  <tr >
    <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
      <div class="one">
        <img src='images/deepfake.jpg' width=100%>
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_AUNet_Learning_Relations_Between_Action_Units_for_Face_Forgery_Detection_CVPR_2023_paper.pdf">
        <span class="papertitle">
          AUNet: Learning Relations Between Action Units for Face Forgery Detection
        </a>
      <br>
      Weiming Bai, Yufan Liu*, <strong>Zhipeng Zhang*</strong>, Bing Li, Weiming Hu

      <br>
      <em>CVPR, 2023</em>
      <br>
      <p>
        <strong>AUNet</strong> proposes the Action-Units Relation Learning framework to improve the generality of forgery (deepfake) detection.
  </tr>
  <!-- ocean -->
  <tr >
    <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
      <div class="one">
        <img src='images/ocean.jpg' width=100%>
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660766.pdf">
        <span class="papertitle">
          Ocean: Object-aware Anchor-free Tracking
        </a>
      <br>
     <strong>Zhipeng Zhang</strong>, Houwen Peng, Jianlong Fu, Bing Li, Weiming Hu

      <br>
      <em>ECCV, 2020 (Cite 900+)</em>
      <br>
      <p>
        <strong>Ocean</strong> explores an efficient anchor-free framework to improve object tracking robustness.
  </tr>

  <!-- siamdw -->
  <tr >
    <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;">
      <div class="one">
        <img src='images/siamdw.jpg' width=100%>
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.pdf">
        <span class="papertitle">
          Deeper and Wider Siamese Networks for Real-Time Visual Tracking
        </a>
      <br>
     <strong>Zhipeng Zhang</strong>, Houwen Peng

      <br>
      <em>CVPR, 2019 (Oral, Cite 1200+)</em>
      <br>
      <p>
        <strong>SiamDW</strong>  is the first work to solve the performance degradation in the Siamese tracking framework when using a deeper network.

</tbody></table>



<!-- ====================== Projects =========================== -->

    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
      <tr>
        <td>
          <h2>Projects</h2>
        </td> -->
      <!-- </tr>
    </tbody></table>
    <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/magicmaker.png" style="width:100%;text-align: center;" ></td>
        <td width="75%" valign="center">
          <a href="https://magicmaker.openxlab.org.cn/home">MagicMaker</a>
          <p>Project Owner, 2023.04 ~ 2024.09</p> -->
          <!-- MagicMaker is a user-friendly AI platform that enables seamless image generation, editing, and animation. It empowers users to transform their imagination into captivating cinema and animations with ease.
          <br>
        </td>
      </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmagic.png" width="200px"></td>
              <td width="75%" valign="center">
                <a href="https://github.com/open-mmlab/mmagic">OpenMMLab/MMagic</a>
                <img src="https://img.shields.io/github/stars/open-mmlab/MMagic?style=social">
                <p>Lead Core Maintainer, 2022.07 ~ 2023.08</p>
                OpenMMLab Multimodal Advanced, Generative, and Intelligent Creation Toolbox. Unlock the magic ü™Ñ: Generative-AI (AIGC), easy-to-use APIs, awsome model zoo, diffusion models, for text-to-image generation, image/video restoration/enhancement, etc.
                <br>
              </td>
            </tr>

        </tbody></table> -->



<!-- =================== working ============================== -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td>
      <h2>Working Experience</h2>
    </td>
  </tr>
</tbody></table>
<table width="100%" align="center" border="0" cellpadding="20"><tbody>
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/sjtu.jpeg" style="width:100%;text-align: center;" ></td>
    <td width="75%" valign="center">
      <a href="https://www.shlab.org.cn/">AI School of Shanghai Jiaotong University</a>
      <p>Assistant Professor, 2025.04</p>
      <br>
    </td>
  </tr>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/kargo2.png" style="width:100%;text-align: center;" ></td>
    <td width="75%" valign="center">
      <a href="https://www.kargo-bot.com/">KargoBot</a>
      <p>Senior Researcher, 2022.07 ~ 2025.03</p>
      <br>
    </td>
  </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/microsoft.png"  style="width:100%;text-align: center;"></td>
          <td width="75%" valign="center">
            <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia (MSRA)</a>
            <p>Research Intern, 2019.08 ~ 2020.06</p>
            <br>
          </td>
        </tr>

    </tbody></table>




<!-- ===================== MISC ============================ -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
      <tr>
        <td>
          <h2>Miscellanea</h2>
        </td>
      </tr>
    </tbody></table>
    <table width="100%" align="center" border="0" cellpadding="10"><tbody>
      <ul>
        <li><b>Conference Reviewer:</b>
          CVPR, ICCV, ECCV, ICML, ICLR, NeurIPS, AAAI.
        </li>
        <li><b>Journal Reviewer:</b>
           T-PAMI, IJCV, TIP, et.al.
        </li>
        <li><b>Invited Talk:</b>
          SiamDW in CVPR2019 (<a href="https://www.bilibili.com/video/BV134411e7g1/?spm_id_from=333.337.search-card.all.click&vd_source=b7baadaad94318165650f979c0973100">ÊûÅÂ∏ÇÂπ≥Âè∞</a>)
        </li>
        <li><b>Invited Talk:</b>
          Ocean in ECCV2020 (<a href="https://www.bilibili.com/video/BV1354y1e7wU/?spm_id_from=333.337.search-card.all.click&vd_source=b7baadaad94318165650f979c0973100">ÊûÅÂ∏ÇÂπ≥Âè∞</a>)
        </li>
        
        <li> <b>Award:</b>
          National Scholarship for Ph.D. </li>
        <li> <b>Award:</b>
            National Scholarship for undergraduate. </li>
        </ul>

  </tbody></table>
        </td>
      </tr>
    </table>


    <!-- <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
        <!-- <td style="width:0%;vertical-align:middle">
        </td>
        <td style="width:100%;vertical-align:middle">
        <hr style="margin-top:0px">
            <p><font color="#999999">The website template was adapted from <a href="https://jonbarron.info/">Jon Barron</a>.</font></p>
        </td> -->
    <!-- </tr> -->
  <!-- </tbody> -->
</table>


</body>
</html>
